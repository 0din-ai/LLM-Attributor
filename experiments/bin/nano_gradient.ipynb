{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f32e98f-c42f-450f-9d04-2aa82a126680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "import math\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import GPTConfig, GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65704e96-4f70-4c0d-a2fd-21a0a1cd0cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"/raid/slee3473/LLM/shakespeare_char/out\"\n",
    "if not os.path.exists(out_dir): raise RuntimeError(f\"No out_dir {out_dir}\")\n",
    "    \n",
    "eval_iters = 200\n",
    "ckpt_iters = 1000\n",
    "log_iters = 100\n",
    "init_from = \"scratch\"  # \"scratch\", \"resume\", or \"gpt2*\"\n",
    "dataset = \"shakespeare_char\" # \"openwebtext\"\n",
    "gradient_accumulation_steps = 1  # 5 * 8  # used to simulate larger batch sizes\n",
    "batch_size = 64  # 12\n",
    "block_size = 256  # 1024\n",
    "\n",
    "n_layer = 6  # 12\n",
    "n_head = 6  # 12\n",
    "n_embed = 384  # 768\n",
    "dropout = 0.2  # 0.0\n",
    "bias = False\n",
    "\n",
    "# AdamW optimizer settings\n",
    "learning_rate = 1e-3  # 6e-4\n",
    "max_iters = 5000  # 600000\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.99  # 0.95\n",
    "grad_clip = 1.0\n",
    "\n",
    "# lr decay settings\n",
    "decay_lr = True \n",
    "warmup_iters = 100  # 2000\n",
    "lr_decay_iters = 5000 # 600000\n",
    "min_lr = 1e-4  # 6e-5\n",
    "\n",
    "# misc\n",
    "device = \"cuda:6\"\n",
    "dtype = \"bfloat16\"\n",
    "compile = True  # use PyTorch 2.0 for faster model compile\n",
    "master_process = True\n",
    "seed_offset = 0\n",
    "ddp_world_size = 1\n",
    "\n",
    "torch.manual_seed(1337+seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True \n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "device_type = \"cuda\"\n",
    "ptdtype = torch.bfloat16  # {\"float32\": torch.float32, \"bfloat16\": torch.bfloat16, \"float16\": torch.float16}[dtype]\n",
    "ctx = torch.amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84c53ede-e8e6-4358-9cd2-6f5188234d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(\"./../nanoGPT/data\", dataset)\n",
    "train_data = np.memmap(os.path.join(data_dir, \"train.bin\"), dtype=np.uint16, mode=\"r\")\n",
    "val_data = np.memmap(os.path.join(data_dir, \"val.bin\"), dtype=np.uint16, mode=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ed98753-c3e0-4672-a942-6e97eecb83fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split ==\"train\" else val_data \n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix]).pin_memory().to(device, non_blocking=True)\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix]).pin_memory().to(device, non_blocking=True)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e4cd5ca-1a0b-4490-bb6f-62ee3e8e3f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found vocab_size = 65 (inside ./../nanoGPT/data/shakespeare_char/meta.pkl)\n"
     ]
    }
   ],
   "source": [
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "meta_path = os.path.join(data_dir, \"meta.pkl\")\n",
    "meta_vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, \"rb\") as f:\n",
    "        meta = pickle.load(f)\n",
    "    meta_vocab_size = meta[\"vocab_size\"]\n",
    "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74a029f5-7a00-45bd-9d9f-628f9f083066",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embed, block_size=block_size, bias=bias, vocab_size=None, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f681c72c-0d6c-4c39-8df5-44c7e8bc78ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 10.65M\n"
     ]
    }
   ],
   "source": [
    "if meta_vocab_size is None:\n",
    "    print(\"Use default vocab_size of GPT-2 (50304 = 50257 rounded up for efficiency)\")\n",
    "model_args[\"vocab_size\"] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
    "gptconf = GPTConfig(**model_args)\n",
    "model = GPT(gptconf).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f803388-dcd2-4cca-8f7e-0f02c2f2a72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 26, with 10,740,096 parameters\n",
      "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype==\"float16\"))\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "checkpoint = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4e2625c-f26b-4cb2-b377-c5c4afad4e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(it):\n",
    "    if it < warmup_iters: return learning_rate * it / warmup_iters\n",
    "    if it > lr_decay_iters: return min_lr \n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1. + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff*(learning_rate-min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a0d232a-e3a1-4927-a45a-9a0065cc47e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15686\n"
     ]
    }
   ],
   "source": [
    "num_train_iters = (len(train_data)+batch_size-1) // batch_size\n",
    "print(num_train_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53d715aa-c8f3-4fbe-8ed1-44452e52a1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def model_eval(model=model):\n",
    "    model.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "        X,Y = get_batch(\"val\")\n",
    "        with ctx: logits, loss = model(X,Y)\n",
    "        losses[k] = loss.item()\n",
    "    model.train()\n",
    "    return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "362373fb-0fe3-41ec-ba25-946c1021434e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 eval loss: 4.2820\n",
      "Iter 0 --- Loss: 4.271435737609863\n",
      "Iter 100 --- Loss: 2.5760741233825684\n",
      "Iter 200 --- Loss: 2.490290880203247\n",
      "Iter 300 --- Loss: 2.4275081157684326\n",
      "Iter 400 --- Loss: 2.338355302810669\n",
      "Iter 500 --- Loss: 2.1652441024780273\n",
      "Iter 600 --- Loss: 1.9487298727035522\n",
      "Iter 700 --- Loss: 1.7803409099578857\n",
      "Iter 800 --- Loss: 1.6560834646224976\n",
      "Iter 900 --- Loss: 1.5977400541305542\n",
      "Iter 1000 --- Loss: 1.5184143781661987\n",
      "Iter 1100 --- Loss: 1.475876808166504\n",
      "Iter 1200 --- Loss: 1.4374158382415771\n",
      "Iter 1300 --- Loss: 1.3937320709228516\n",
      "Iter 1400 --- Loss: 1.3578182458877563\n",
      "Iter 1500 --- Loss: 1.363014578819275\n",
      "Iter 1600 --- Loss: 1.3122446537017822\n",
      "Iter 1700 --- Loss: 1.2949780225753784\n",
      "Iter 1800 --- Loss: 1.2825500965118408\n",
      "Iter 1900 --- Loss: 1.2612237930297852\n",
      "Iter 2000 --- Loss: 1.2471721172332764\n",
      "Iter 2100 --- Loss: 1.2602064609527588\n",
      "Iter 2200 --- Loss: 1.2254849672317505\n",
      "Iter 2300 --- Loss: 1.2261593341827393\n",
      "Iter 2400 --- Loss: 1.2354092597961426\n",
      "Iter 2500 --- Loss: 1.171285629272461\n",
      "Iter 2600 --- Loss: 1.1648353338241577\n",
      "Iter 2700 --- Loss: 1.1754578351974487\n",
      "Iter 2800 --- Loss: 1.182364583015442\n",
      "Iter 2900 --- Loss: 1.1734960079193115\n",
      "Iter 3000 --- Loss: 1.136385202407837\n",
      "Iter 3100 --- Loss: 1.1289089918136597\n",
      "Iter 3200 --- Loss: 1.1273870468139648\n",
      "Iter 3300 --- Loss: 1.1360204219818115\n",
      "Iter 3400 --- Loss: 1.1197415590286255\n",
      "Iter 3500 --- Loss: 1.1259214878082275\n",
      "Iter 3600 --- Loss: 1.1108922958374023\n",
      "Iter 3700 --- Loss: 1.0987522602081299\n",
      "Iter 3800 --- Loss: 1.1075786352157593\n",
      "Iter 3900 --- Loss: 1.0796444416046143\n",
      "Iter 4000 --- Loss: 1.0634257793426514\n",
      "Iter 4100 --- Loss: 1.0589834451675415\n",
      "Iter 4200 --- Loss: 1.0238194465637207\n",
      "Iter 4300 --- Loss: 1.0304750204086304\n",
      "Iter 4400 --- Loss: 1.0182362794876099\n",
      "Iter 4500 --- Loss: 1.0264840126037598\n",
      "Iter 4600 --- Loss: 1.0289732217788696\n",
      "Iter 4700 --- Loss: 0.9850641489028931\n",
      "Iter 4800 --- Loss: 0.989805281162262\n",
      "Iter 4900 --- Loss: 0.9729213118553162\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "num_train_iters = (len(train_data)-block_size+batch_size-1) // batch_size\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # start eval\n",
    "    loss = model_eval()\n",
    "    print(f\"Epoch {epoch} eval loss: {loss:.4f}\")\n",
    "\n",
    "    indices = torch.randperm(len(train_data)-block_size)\n",
    "    \n",
    "    # start training\n",
    "    model.train()\n",
    "    for iter_i in range(max_iters):\n",
    "        if iter_i%ckpt_iters==0:\n",
    "            checkpoint = {\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'model_args': model_args,\n",
    "            }\n",
    "            torch.save(checkpoint, os.path.join(out_dir, f\"ckpt_{iter_i}.pt\"))\n",
    "        ix = indices[iter_i*batch_size:(iter_i+1)*batch_size]\n",
    "        X = torch.stack([torch.from_numpy((train_data[i:i+block_size]).astype(np.int64)) for i in ix]).pin_memory().to(device, non_blocking=True)\n",
    "        Y = torch.stack([torch.from_numpy((train_data[i+1:i+1+block_size]).astype(np.int64)) for i in ix]).pin_memory().to(device, non_blocking=True)\n",
    "        \n",
    "        with ctx:\n",
    "            logits, loss = model(X,Y)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        if iter_i % log_iters == 0: print(f\"Iter {iter_i} --- Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "918272b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = {\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'model_args': model_args,   \n",
    "}\n",
    "torch.save(checkpoint, os.path.join(out_dir, f\"final.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4d958dd-4117-4cf1-ac45-1734ef8fad3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training val loss: 1.5941\n"
     ]
    }
   ],
   "source": [
    "print(f\"After training val loss: {model_eval():.4f}\")  # 1.5941"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e27eb0",
   "metadata": {},
   "source": [
    "### Load saved final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25c9c809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 10.65M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ckpt = torch.load(os.path.join(out_dir, \"final_4999.pt\"), map_location=device)\n",
    "    \n",
    "state_dict = model_ckpt[\"model\"]\n",
    "model_args = model_ckpt[\"model_args\"]\n",
    "opt_args = model_ckpt[\"optimizer\"]\n",
    "    \n",
    "model_config = GPTConfig(**model_args)\n",
    "model = GPT(model_config).to(device)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "095df7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model loss: 1.5832\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print(f\"Loaded model loss: {model_eval():.4f}\")  # 1.5832"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50cc073-dd55-4938-9b1f-d7c2023d73c0",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "821944ab-ef4d-486c-a6b9-bf7bf4d3dc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, \"meta.pkl\"), \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccee0704-487b-4881-abd8-f7528d8e3aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = \"\\n\"\n",
    "start_ids = encode(start)\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ede4572-7d37-4ed8-ac9c-7ee9005e474b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Clown:\n",
      "So is it and in all thing that was to banishment\n",
      "what taught the prince hath been with him.\n",
      "\n",
      "AUTOLYCUS:\n",
      "Hand, I hope he of these five consulships!\n",
      "\n",
      "AUTOLYCUS:\n",
      "I will that Hollaud by the common\n",
      "====\n",
      "\n",
      "Behold you all less to expose you our grace!\n",
      "\n",
      "KING EDWARD IV:\n",
      "Thanks. But, tell me, do not see the king.\n",
      "\n",
      "GLOUCESTER:\n",
      "The earth shall be punish'd, poor gentle lady.\n",
      "\n",
      "LADY GREY:\n",
      "I'll swear with thee in\n",
      "====\n",
      "\n",
      "Men at the dark not of my fair bears,\n",
      "Nor warm'd all the great confined cares:\n",
      "So reprove a loving credent soul\n",
      "Is your father breaths; and you might be\n",
      "Down this daughter's blood. You are both.\n",
      "\n",
      "TYBA\n",
      "====\n",
      "\n",
      "That parts the might of England's peace\n",
      "And wear the king at the palsied mother,\n",
      "Or that ever the hour and means to be\n",
      "Maligation that to loud to the old world,\n",
      "When we are parcell'd for so villain,\n",
      "F\n",
      "====\n",
      "\n",
      "What said 'twas this and 'We can, he shall be past.'\n",
      "\n",
      "Second Watchman:\n",
      "Who is't not? why, no man take it you out;\n",
      "But no more of this world, marry, more cordial.\n",
      "\n",
      "Third Watchman:\n",
      "The first confirm of \n",
      "====\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(5):\n",
    "            y = model.generate(x, 200, temperature=0.8, top_k=200)\n",
    "            print(decode(y[0].tolist()))\n",
    "            print(\"====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d3fe8c-d103-4237-87e0-730f872bb98c",
   "metadata": {},
   "source": [
    "### Load checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34666fca-7079-4817-bd15-9695947b0fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckpt(ckpt_name=\"ckpt.pt\", ckpt_dir=out_dir, device=device):\n",
    "    ckpt = torch.load(os.path.join(ckpt_dir, ckpt_name), map_location=device)\n",
    "    \n",
    "    ckpt_state_dict = ckpt[\"model\"]\n",
    "    ckpt_model_args = ckpt[\"model_args\"]\n",
    "    ckpt_opt_args = ckpt[\"optimizer\"]\n",
    "    \n",
    "    ckpt_config = GPTConfig(**ckpt_model_args)\n",
    "    ckpt_model = GPT(ckpt_config).to(device)\n",
    "    ckpt_model.load_state_dict(ckpt_state_dict)\n",
    "\n",
    "    print(f\"Loaded checkpoint model {ckpt_name}\")\n",
    "\n",
    "    return ckpt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05368ed3-da00-4789-9c5b-f320d03ede6c",
   "metadata": {},
   "source": [
    "### Generate something to be attributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a57c1d45-2935-484c-bdab-43fc4e3ad561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Romeo, and\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "x = (torch.tensor(encode(\"\\n\"), dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        out = model.generate(x, 10, temperature=1., top_k=200)\n",
    "        print(decode(out[0].tolist()))\n",
    "        print(\"====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d428138-1670-4bf1-b37a-649076619c05",
   "metadata": {},
   "source": [
    "### Attribute using Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "057859ae-4850-4325-97a8-7f5f87c02b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_X = (torch.tensor(encode(\"\\nRome\"), dtype=torch.long, device=device)[None, ...])\n",
    "attr_Y = (torch.tensor(encode(\"o\"), dtype=torch.long, device=device)[None, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8d179ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of o to come after Rome: 0.9756704568862915\n",
      "Probability of space to come after Rome: 0.012636329047381878\n",
      "Probability of newline to come after Rome: 1.668629192863591e-05\n"
     ]
    }
   ],
   "source": [
    "# probability of 'o' to come after 'Rome'\n",
    "model.eval()\n",
    "\n",
    "attr_logits, _ = model(attr_X)\n",
    "prob = F.softmax(attr_logits, dim=-1)\n",
    "print(\"Probability of o to come after Rome:\", prob[0,0,attr_Y.item()].item())\n",
    "\n",
    "tensor_space = (torch.tensor(encode(\" \"), dtype=torch.long, device=device)[None, ...])\n",
    "print(\"Probability of space to come after Rome:\", prob[0,0,tensor_space.item()].item())\n",
    "\n",
    "tensor_newline = (torch.tensor(encode(\"\\n\"), dtype=torch.long, device=device)[None, ...])\n",
    "print(\"Probability of newline to come after Rome:\", prob[0,0,tensor_newline.item()].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9af0175-18cd-4e32-8e7c-58da231c79bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_gradient(idx, targets, model=model):\n",
    "    b, t = idx.size()\n",
    "    pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "    # forward the GPT model itself\n",
    "    tok_emb = model.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "    pos_emb = model.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "    x = model.transformer.drop(tok_emb + pos_emb)\n",
    "    for block in model.transformer.h:\n",
    "        x = block(x)\n",
    "    x = model.transformer.ln_f(x)\n",
    "\n",
    "    logits = model.lm_head(x)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)    \n",
    "    \n",
    "    # tok_emb.requires_grad = True\n",
    "    tok_emb.retain_grad()\n",
    "\n",
    "    gradient = torch.autograd.grad(loss, tok_emb, create_graph=True)[0]  # batch_size x block_size x n_embed\n",
    "    print(gradient.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbdebbbe-b94a-4bf9-92fa-356fb107cb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple_inner_product(t1, t2):\n",
    "    assert len(t1) == len(t2)\n",
    "\n",
    "    inner = 0\n",
    "    for d1, d2 in zip(t1, t2):\n",
    "        assert torch.numel(d1) == torch.numel(d2)\n",
    "        inner += torch.inner(d1.flatten(), d2.flatten())\n",
    "\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ef9320f-53a3-427a-97d7-5ed64bc54877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time until iter 0: 0.0002sec\n",
      "number of parameters: 10.65M\n",
      "Loaded checkpoint model ckpt_0.pt\n",
      "Time until iter 100: 49.6408sec\n",
      "Time until iter 200: 99.2457sec\n",
      "Time until iter 300: 148.5584sec\n",
      "Time until iter 400: 197.9722sec\n",
      "Time until iter 500: 247.5307sec\n",
      "Time until iter 600: 296.8218sec\n",
      "Time until iter 700: 346.5458sec\n",
      "Time until iter 800: 396.0292sec\n",
      "Time until iter 900: 445.4531sec\n",
      "Time until iter 1000: 495.0766sec\n",
      "number of parameters: 10.65M\n",
      "Loaded checkpoint model ckpt_1000.pt\n",
      "Time until iter 1100: 544.6561sec\n",
      "Time until iter 1200: 593.9342sec\n",
      "Time until iter 1300: 643.4771sec\n",
      "Time until iter 1400: 692.7682sec\n",
      "Time until iter 1500: 742.0047sec\n",
      "Time until iter 1600: 791.4791sec\n",
      "Time until iter 1700: 840.6524sec\n",
      "Time until iter 1800: 890.1010sec\n",
      "Time until iter 1900: 939.3306sec\n",
      "Time until iter 2000: 987.7405sec\n",
      "number of parameters: 10.65M\n",
      "Loaded checkpoint model ckpt_2000.pt\n",
      "Time until iter 2100: 1037.1159sec\n",
      "Time until iter 2200: 1086.6880sec\n",
      "Time until iter 2300: 1135.5715sec\n",
      "Time until iter 2400: 1184.0753sec\n",
      "Time until iter 2500: 1232.2967sec\n",
      "Time until iter 2600: 1280.5550sec\n",
      "Time until iter 2700: 1329.0346sec\n",
      "Time until iter 2800: 1377.1951sec\n",
      "Time until iter 2900: 1425.5668sec\n",
      "Time until iter 3000: 1473.7800sec\n",
      "number of parameters: 10.65M\n",
      "Loaded checkpoint model ckpt_3000.pt\n",
      "Time until iter 3100: 1522.0264sec\n",
      "Time until iter 3200: 1570.2683sec\n",
      "Time until iter 3300: 1618.3480sec\n",
      "Time until iter 3400: 1666.6278sec\n",
      "Time until iter 3500: 1715.0450sec\n",
      "Time until iter 3600: 1763.7000sec\n",
      "Time until iter 3700: 1812.1283sec\n",
      "Time until iter 3800: 1857.5860sec\n",
      "Time until iter 3900: 1902.7191sec\n",
      "Time until iter 4000: 1947.6027sec\n",
      "number of parameters: 10.65M\n",
      "Loaded checkpoint model ckpt_4000.pt\n",
      "Time until iter 4100: 1995.7887sec\n",
      "Time until iter 4200: 2042.5912sec\n",
      "Time until iter 4300: 2087.2909sec\n",
      "Time until iter 4400: 2132.0874sec\n",
      "Time until iter 4500: 2176.8777sec\n",
      "Time until iter 4600: 2221.6177sec\n",
      "Time until iter 4700: 2270.4215sec\n",
      "Time until iter 4800: 2319.4583sec\n",
      "Time until iter 4900: 2365.3002sec\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "influence_scores = np.zeros(len(train_data))\n",
    "start = time.time()\n",
    "time_log_iters = 100\n",
    "\n",
    "for iter_i in range(max_iters):\n",
    "    if iter_i % time_log_iters == 0:\n",
    "        duration = time.time() - start \n",
    "        print(f\"Time until iter {iter_i}: {duration:.4f}sec\")\n",
    "    if iter_i % ckpt_iters == 0:\n",
    "        ckpt_model = load_ckpt(ckpt_name=f\"ckpt_{iter_i}.pt\")\n",
    "        ckpt_model.eval()\n",
    "        \n",
    "        attr_logits, _ = ckpt_model(attr_X)\n",
    "        attr_logits = attr_logits[:,-1,:] / 1. # temperature\n",
    "        v, _ = torch.topk(attr_logits, min(200, attr_logits.size(-1)))\n",
    "        attr_logits[attr_logits < v[:,[-1]]] = -float(\"Inf\")\n",
    "        probs = F.softmax(attr_logits, dim=-1)\n",
    "        attr_Y_prob = probs[0, attr_Y.item()]\n",
    "        attr_Y_prob_gradient = torch.autograd.grad(attr_Y_prob, ckpt_model.parameters())\n",
    "\n",
    "    for i in indices[iter_i*batch_size:(iter_i+1)*batch_size]:\n",
    "        X = torch.unsqueeze(torch.from_numpy((train_data[i:i+block_size]).astype(np.int64)).pin_memory().to(device, non_blocking=True), 0)\n",
    "        Y = torch.unsqueeze(torch.from_numpy((train_data[i+1:i+1+block_size]).astype(np.int64)).pin_memory().to(device, non_blocking=True), 0)\n",
    "        logits, loss = ckpt_model(X, Y)\n",
    "        training_gradient = torch.autograd.grad(loss, ckpt_model.parameters())\n",
    "        influence_scores[i] = tuple_inner_product(training_gradient, attr_Y_prob_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ed709739-5f41-46ed-92d3-c1ccfb176a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[483533, 742877, 42431, 483529, 42425, 742895, 483573, 742883, 742874, 514847, 42498, 670758, 60369, 670748, 483574, 42432, 483566, 670737, 483559, 670767, 742870, 514895, 742793, 483672, 42456, 670756, 505991, 42416, 742873, 483681, 60246, 561191, 742851, 483589, 398928, 60251, 765767, 235250, 140424, 670703, 670774, 561196, 397207, 483523, 483596, 230683, 504530, 801169, 561206, 60357, 742910, 670778, 848842, 230677, 742806, 32683, 765809, 54215, 54194, 42409, 483620, 742789, 13932, 483684]\n",
      "[-10.38266182 -10.20835876  -9.94726467  -9.86074352  -9.79647636\n",
      "  -9.58438206  -9.50465202  -9.45061588  -9.41490269  -9.39520741\n",
      "  -9.36907196  -9.36326408  -9.31201935  -9.29672718  -9.27636433\n",
      "  -9.26184273  -9.22461033  -9.20440483  -9.15724754  -9.12469673\n",
      "  -9.09008408  -8.98343277  -8.93583107  -8.87154293  -8.84836388\n",
      "  -8.84537888  -8.84458256  -8.77223015  -8.75917339  -8.75551224\n",
      "  -8.72925758  -8.70864677  -8.67463684  -8.65811539  -8.58307934\n",
      "  -8.56818199  -8.56700516  -8.54398537  -8.53158379  -8.52946281\n",
      "  -8.51904011  -8.51347923  -8.48208904  -8.45854568  -8.45404625\n",
      "  -8.41749859  -8.41565418  -8.41505718  -8.39440918  -8.39261246\n",
      "  -8.38960743  -8.37764359  -8.37727833  -8.34565735  -8.32059288\n",
      "  -8.31597996  -8.30229664  -8.29876804  -8.26815128  -8.24707413\n",
      "  -8.22574902  -8.17130947  -8.16116524  -8.15581703]\n"
     ]
    }
   ],
   "source": [
    "N = 64\n",
    "cnt = 0\n",
    "influnece_scores_ = influence_scores.copy()\n",
    "topN_pos_influence = []  # 483533, 742877, 42431, 514847, 670758\n",
    "\n",
    "while True:\n",
    "    if cnt == N: break\n",
    "\n",
    "    top_influence_idx = np.argmin(influnece_scores_)\n",
    "    topN_pos_influence.append(top_influence_idx)\n",
    "    cnt += 1\n",
    "#     influnece_scores_[top_influence_idx-block_size+1:top_influence_idx+block_size] = float(\"Inf\")\n",
    "    influnece_scores_[top_influence_idx] = float(\"Inf\")\n",
    "\n",
    "print(topN_pos_influence)\n",
    "print(influence_scores[topN_pos_influence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20ec9c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[670410, 837416, 662730, 157434, 796033]\n",
      "[2.6630044  2.54692841 2.30066371 2.06376839 1.96041548]\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "cnt = 0\n",
    "influnece_scores_ = influence_scores.copy()\n",
    "topN_neg_influence = []\n",
    "\n",
    "while True:\n",
    "    if cnt == N: break\n",
    "\n",
    "    top_influence_idx = np.argmax(influnece_scores_)\n",
    "    topN_neg_influence.append(top_influence_idx)\n",
    "    cnt += 1\n",
    "    influnece_scores_[top_influence_idx-block_size+1:top_influence_idx+block_size] = -float(\"Inf\")\n",
    "\n",
    "print(topN_neg_influence)\n",
    "print(influence_scores[topN_neg_influence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b7e09e97-5149-4598-ad1f-cf0ae22dd8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<< POSITIVE INFLUENCE >>>\n",
      "0.\n",
      ":\n",
      "To-morrow will I send.\n",
      "\n",
      "ROMEO:\n",
      "So thrive my soul--\n",
      "\n",
      "JULIET:\n",
      "A thousand times good night!\n",
      "\n",
      "ROMEO:\n",
      "A thousand times the worse, to want thy light.\n",
      "Love goes toward love, as schoolboys from\n",
      "their books,\n",
      "But love from love, toward school with heavy looks.\n",
      "\n",
      "JUL\n",
      "==================\n",
      "1.\n",
      "who I am.\n",
      "Good lady,\n",
      "No court in Europe is too good for thee;\n",
      "What dost thou then in prison?\n",
      "Now, good sir,\n",
      "You know me, do you not?\n",
      "\n",
      "Gaoler:\n",
      "For a worthy lady\n",
      "And one whom much I honour.\n",
      "\n",
      "PAULINA:\n",
      "Pray you then,\n",
      "Conduct me to the queen.\n",
      "\n",
      "Gaoler:\n",
      "I may not,\n",
      "==================\n",
      "2.\n",
      "our follows Coriolanus.\n",
      "Welcome to Rome, renowned Coriolanus!\n",
      "\n",
      "All:\n",
      "Welcome to Rome, renowned Coriolanus!\n",
      "\n",
      "CORIOLANUS:\n",
      "No more of this; it does offend my heart:\n",
      "Pray now, no more.\n",
      "\n",
      "COMINIUS:\n",
      "Look, sir, your mother!\n",
      "\n",
      "CORIOLANUS:\n",
      "O,\n",
      "You have, I know, petition\n",
      "==================\n",
      "3.\n",
      "rief:\n",
      "To-morrow will I send.\n",
      "\n",
      "ROMEO:\n",
      "So thrive my soul--\n",
      "\n",
      "JULIET:\n",
      "A thousand times good night!\n",
      "\n",
      "ROMEO:\n",
      "A thousand times the worse, to want thy light.\n",
      "Love goes toward love, as schoolboys from\n",
      "their books,\n",
      "But love from love, toward school with heavy looks.\n",
      "\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "print(\"<<< POSITIVE INFLUENCE >>>\")\n",
    "for cnt, i in enumerate(topN_pos_influence):\n",
    "    print(f\"{cnt}.\")\n",
    "    print(decode(train_data[i:i+block_size+1]))\n",
    "    print(\"==================\")\n",
    "    \n",
    "    if cnt == 3: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e480c9e",
   "metadata": {},
   "source": [
    "### Exclude topN_pos examples and retrain the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56206b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319936\n"
     ]
    }
   ],
   "source": [
    "removed_indices = indices[:max_iters*batch_size].numpy().copy()\n",
    "for i in topN_pos_influence:\n",
    "    removed_indices = np.delete(removed_indices, removed_indices==i)\n",
    "print(len(removed_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "25948ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 10.65M\n"
     ]
    }
   ],
   "source": [
    "new_model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embed, block_size=block_size, bias=bias, vocab_size=None, dropout=dropout)\n",
    "new_model_args[\"vocab_size\"] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
    "new_gptconf = GPTConfig(**new_model_args)\n",
    "new_model = GPT(new_gptconf).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fc338aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 26, with 10,740,096 parameters\n",
      "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype==\"float16\"))\n",
    "new_optimizer = new_model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9450518c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 eval loss: 4.3246\n",
      "Iter 0 --- Loss: 4.3122968673706055\n",
      "Iter 100 --- Loss: 2.551508903503418\n",
      "Iter 200 --- Loss: 2.4681930541992188\n",
      "Iter 300 --- Loss: 2.3844492435455322\n",
      "Iter 400 --- Loss: 2.169111728668213\n",
      "Iter 500 --- Loss: 1.9566351175308228\n",
      "Iter 600 --- Loss: 1.8073127269744873\n",
      "Iter 700 --- Loss: 1.7086583375930786\n",
      "Iter 800 --- Loss: 1.559480905532837\n",
      "Iter 900 --- Loss: 1.523234248161316\n",
      "Iter 1000 --- Loss: 1.4675263166427612\n",
      "Iter 1100 --- Loss: 1.406903624534607\n",
      "Iter 1200 --- Loss: 1.4059370756149292\n",
      "Iter 1300 --- Loss: 1.3432624340057373\n",
      "Iter 1400 --- Loss: 1.3720996379852295\n",
      "Iter 1500 --- Loss: 1.3468955755233765\n",
      "Iter 1600 --- Loss: 1.2828428745269775\n",
      "Iter 1700 --- Loss: 1.2782714366912842\n",
      "Iter 1800 --- Loss: 1.288665771484375\n",
      "Iter 1900 --- Loss: 1.2476208209991455\n",
      "Iter 2000 --- Loss: 1.2477058172225952\n",
      "Iter 2100 --- Loss: 1.2114722728729248\n",
      "Iter 2200 --- Loss: 1.2032109498977661\n",
      "Iter 2300 --- Loss: 1.160823941230774\n",
      "Iter 2400 --- Loss: 1.1823413372039795\n",
      "Iter 2500 --- Loss: 1.1622190475463867\n",
      "Iter 2600 --- Loss: 1.1623226404190063\n",
      "Iter 2700 --- Loss: 1.1332347393035889\n",
      "Iter 2800 --- Loss: 1.1474987268447876\n",
      "Iter 2900 --- Loss: 1.1263269186019897\n",
      "Iter 3000 --- Loss: 1.1247614622116089\n",
      "Iter 3100 --- Loss: 1.119560956954956\n",
      "Iter 3200 --- Loss: 1.068076491355896\n",
      "Iter 3300 --- Loss: 1.085273265838623\n",
      "Iter 3400 --- Loss: 1.0796951055526733\n",
      "Iter 3500 --- Loss: 1.0727262496948242\n",
      "Iter 3600 --- Loss: 1.0675908327102661\n",
      "Iter 3700 --- Loss: 1.022601842880249\n",
      "Iter 3800 --- Loss: 1.0653327703475952\n",
      "Iter 3900 --- Loss: 1.0221726894378662\n",
      "Iter 4000 --- Loss: 1.0284674167633057\n",
      "Iter 4100 --- Loss: 1.026205062866211\n",
      "Iter 4200 --- Loss: 1.0166889429092407\n",
      "Iter 4300 --- Loss: 0.994592547416687\n",
      "Iter 4400 --- Loss: 0.9899289011955261\n",
      "Iter 4500 --- Loss: 0.9955753087997437\n",
      "Iter 4600 --- Loss: 0.988320529460907\n",
      "Iter 4700 --- Loss: 0.9411286115646362\n",
      "Iter 4800 --- Loss: 0.9212101697921753\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects a non-empty TensorList",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iter_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     12\u001b[0m     ix \u001b[38;5;241m=\u001b[39m removed_indices[iter_i\u001b[38;5;241m*\u001b[39mbatch_size:\u001b[38;5;28mmin\u001b[39m((iter_i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mbatch_size, \u001b[38;5;28mlen\u001b[39m(indices))]\n\u001b[0;32m---> 13\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mix\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpin_memory()\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m     Y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([torch\u001b[38;5;241m.\u001b[39mfrom_numpy((train_data[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mblock_size])\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint64)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\u001b[38;5;241m.\u001b[39mpin_memory()\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "num_train_iters = (len(train_data)-block_size+batch_size-1) // batch_size\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # start eval\n",
    "    loss = model_eval(new_model)\n",
    "    print(f\"Epoch {epoch} eval loss: {loss:.4f}\")\n",
    "    \n",
    "    # start training\n",
    "    new_model.train()\n",
    "    for iter_i in range(max_iters-1):\n",
    "        ix = removed_indices[iter_i*batch_size:min((iter_i+1)*batch_size, len(indices))]\n",
    "        X = torch.stack([torch.from_numpy((train_data[i:i+block_size]).astype(np.int64)) for i in ix]).pin_memory().to(device, non_blocking=True)\n",
    "        Y = torch.stack([torch.from_numpy((train_data[i+1:i+1+block_size]).astype(np.int64)) for i in ix]).pin_memory().to(device, non_blocking=True)\n",
    "        \n",
    "        with ctx:\n",
    "            logits, loss = new_model(X,Y)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(new_model.parameters(), grad_clip)\n",
    "        new_optimizer.step()\n",
    "        new_optimizer.zero_grad(set_to_none=True)\n",
    "        if iter_i % log_iters == 0: print(f\"Iter {iter_i} --- Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43e6284",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"After training val loss: {model_eval(new_model):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84867111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability of 'o' to come after 'Rome'\n",
    "new_model.eval()\n",
    "\n",
    "attr_logits, _ = new_model(attr_X)\n",
    "prob = F.softmax(attr_logits, dim=-1)\n",
    "print(\"Probability of o to come after Rome:\", prob[0,0,attr_Y.item()].item())\n",
    "\n",
    "tensor_space = (torch.tensor(encode(\" \"), dtype=torch.long, device=device)[None, ...])\n",
    "print(\"Probability of space to come after Rome:\", prob[0,0,tensor_space.item()].item())\n",
    "\n",
    "tensor_newline = (torch.tensor(encode(\"\\n\"), dtype=torch.long, device=device)[None, ...])\n",
    "print(\"Probability of newline to come after Rome:\", prob[0,0,tensor_newline.item()].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8dec75",
   "metadata": {},
   "source": [
    "### Remove data with \"Romeo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f2bfb98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 320000/320000 [03:09<00:00, 1691.96it/s]\n"
     ]
    }
   ],
   "source": [
    "removed_indices = indices[:max_iters*batch_size].numpy().copy()\n",
    "for i in tqdm(indices.cpu().numpy()[:max_iters*batch_size]):\n",
    "    text = decode(train_data[i:i+block_size+1])\n",
    "    if \"Romeo\" in text:\n",
    "        removed_indices = np.delete(removed_indices, np.where(removed_indices==i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "49ca60cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312193\n"
     ]
    }
   ],
   "source": [
    "print(len(removed_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "82fde6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 10.65M\n"
     ]
    }
   ],
   "source": [
    "new_model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embed, block_size=block_size, bias=bias, vocab_size=None, dropout=dropout)\n",
    "new_model_args[\"vocab_size\"] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
    "new_gptconf = GPTConfig(**new_model_args)\n",
    "new_model = GPT(new_gptconf).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f8c803d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 26, with 10,740,096 parameters\n",
      "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype==\"float16\"))\n",
    "new_optimizer = new_model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "20b5c6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 eval loss: 4.2766\n",
      "Iter 0 --- Loss: 4.263223648071289\n",
      "Iter 100 --- Loss: 2.6077616214752197\n",
      "Iter 200 --- Loss: 2.4815001487731934\n",
      "Iter 300 --- Loss: 2.4124205112457275\n",
      "Iter 400 --- Loss: 2.244668483734131\n",
      "Iter 500 --- Loss: 2.0451645851135254\n",
      "Iter 600 --- Loss: 1.8490885496139526\n",
      "Iter 700 --- Loss: 1.7282294034957886\n",
      "Iter 800 --- Loss: 1.589887022972107\n",
      "Iter 900 --- Loss: 1.5398356914520264\n",
      "Iter 1000 --- Loss: 1.4945701360702515\n",
      "Iter 1100 --- Loss: 1.444478988647461\n",
      "Iter 1200 --- Loss: 1.4442970752716064\n",
      "Iter 1300 --- Loss: 1.3724384307861328\n",
      "Iter 1400 --- Loss: 1.3988631963729858\n",
      "Iter 1500 --- Loss: 1.3652894496917725\n",
      "Iter 1600 --- Loss: 1.2999608516693115\n",
      "Iter 1700 --- Loss: 1.2941566705703735\n",
      "Iter 1800 --- Loss: 1.301592469215393\n",
      "Iter 1900 --- Loss: 1.2709195613861084\n",
      "Iter 2000 --- Loss: 1.263702154159546\n",
      "Iter 2100 --- Loss: 1.2355328798294067\n",
      "Iter 2200 --- Loss: 1.2279256582260132\n",
      "Iter 2300 --- Loss: 1.1929025650024414\n",
      "Iter 2400 --- Loss: 1.2127612829208374\n",
      "Iter 2500 --- Loss: 1.201539397239685\n",
      "Iter 2600 --- Loss: 1.1829992532730103\n",
      "Iter 2700 --- Loss: 1.1639431715011597\n",
      "Iter 2800 --- Loss: 1.187659740447998\n",
      "Iter 2900 --- Loss: 1.1611822843551636\n",
      "Iter 3000 --- Loss: 1.1503509283065796\n",
      "Iter 3100 --- Loss: 1.1530711650848389\n",
      "Iter 3200 --- Loss: 1.1001636981964111\n",
      "Iter 3300 --- Loss: 1.1247683763504028\n",
      "Iter 3400 --- Loss: 1.1293023824691772\n",
      "Iter 3500 --- Loss: 1.1112418174743652\n",
      "Iter 3600 --- Loss: 1.0912110805511475\n",
      "Iter 3700 --- Loss: 1.0624958276748657\n",
      "Iter 3800 --- Loss: 1.0888844728469849\n",
      "Iter 3900 --- Loss: 1.0478308200836182\n",
      "Iter 4000 --- Loss: 1.0534027814865112\n",
      "Iter 4100 --- Loss: 1.052046298980713\n",
      "Iter 4200 --- Loss: 1.0448522567749023\n",
      "Iter 4300 --- Loss: 1.0268471240997314\n",
      "Iter 4400 --- Loss: 1.025526523590088\n",
      "Iter 4500 --- Loss: 1.0247787237167358\n",
      "Iter 4600 --- Loss: 1.0280705690383911\n",
      "Iter 4700 --- Loss: 0.9805669188499451\n",
      "Iter 4800 --- Loss: 0.9462859630584717\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects a non-empty TensorList",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iter_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[1;32m     12\u001b[0m     ix \u001b[38;5;241m=\u001b[39m removed_indices[iter_i\u001b[38;5;241m*\u001b[39mbatch_size:\u001b[38;5;28mmin\u001b[39m((iter_i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mbatch_size, \u001b[38;5;28mlen\u001b[39m(indices))]\n\u001b[0;32m---> 13\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mix\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpin_memory()\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m     Y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([torch\u001b[38;5;241m.\u001b[39mfrom_numpy((train_data[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mblock_size])\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint64)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\u001b[38;5;241m.\u001b[39mpin_memory()\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "num_train_iters = (len(train_data)-block_size+batch_size-1) // batch_size\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # start eval\n",
    "    loss = model_eval(new_model)\n",
    "    print(f\"Epoch {epoch} eval loss: {loss:.4f}\")\n",
    "    \n",
    "    # start training\n",
    "    new_model.train()\n",
    "    for iter_i in range(max_iters):\n",
    "        ix = removed_indices[iter_i*batch_size:min((iter_i+1)*batch_size, len(indices))]\n",
    "        X = torch.stack([torch.from_numpy((train_data[i:i+block_size]).astype(np.int64)) for i in ix]).pin_memory().to(device, non_blocking=True)\n",
    "        Y = torch.stack([torch.from_numpy((train_data[i+1:i+1+block_size]).astype(np.int64)) for i in ix]).pin_memory().to(device, non_blocking=True)\n",
    "        \n",
    "        with ctx:\n",
    "            logits, loss = new_model(X,Y)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(new_model.parameters(), grad_clip)\n",
    "        new_optimizer.step()\n",
    "        new_optimizer.zero_grad(set_to_none=True)\n",
    "        if iter_i % log_iters == 0: print(f\"Iter {iter_i} --- Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a9289cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of o to come after Rome: 1.1718222594936378e-05\n",
      "Probability of space to come after Rome: 0.31933608651161194\n",
      "Probability of newline to come after Rome: 0.0013829857343807817\n"
     ]
    }
   ],
   "source": [
    "# probability of 'o' to come after 'Rome'\n",
    "new_model.eval()\n",
    "\n",
    "attr_logits, _ = new_model(attr_X)\n",
    "prob = F.softmax(attr_logits, dim=-1)\n",
    "print(\"Probability of o to come after Rome:\", prob[0,0,attr_Y.item()].item())\n",
    "\n",
    "tensor_space = (torch.tensor(encode(\" \"), dtype=torch.long, device=device)[None, ...])\n",
    "print(\"Probability of space to come after Rome:\", prob[0,0,tensor_space.item()].item())\n",
    "\n",
    "tensor_newline = (torch.tensor(encode(\"\\n\"), dtype=torch.long, device=device)[None, ...])\n",
    "print(\"Probability of newline to come after Rome:\", prob[0,0,tensor_newline.item()].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8b229c",
   "metadata": {},
   "source": [
    "### Investigate the attributed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "21494c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data # 30414 : 483533\n",
      "Training data # 37839 : 742877\n",
      "Training data # 54651 : 42431\n",
      "Training data # 25854 : 483529\n",
      "Training data # 23214 : 42425\n",
      "Training data # 51840 : 742895\n",
      "Training data # 52988 : 483573\n",
      "Training data # 34977 : 742883\n",
      "Training data # 22401 : 742874\n",
      "Training data # 33611 : 514847\n"
     ]
    }
   ],
   "source": [
    "for i in topN_pos_influence[:10]:\n",
    "    print(\"Training data #\", np.where(indices==i)[0][0], \":\", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27e2ca6",
   "metadata": {},
   "source": [
    "### Investigate model embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "448bc726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 384])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_embedding(idx):\n",
    "    b, t = idx.size()\n",
    "    pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "    # forward the GPT model itself\n",
    "    tok_emb = model.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "    pos_emb = model.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "    x = model.transformer.drop(tok_emb + pos_emb)\n",
    "    for block in model.transformer.h:\n",
    "        x = block(x)\n",
    "    x = model.transformer.ln_f(x)\n",
    "\n",
    "    print(x.shape)\n",
    "\n",
    "get_model_embedding(attr_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c7bef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f7bfce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b793bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a29509f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544b44ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c9122e-70b7-4eca-8c10-b997ae970916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fcafad-922c-4f98-90d4-e0e29c0dc70f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
