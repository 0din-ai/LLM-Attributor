{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c913e897-f341-4369-8796-52882544febe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load a fine-tuned model from /raid/slee3473/LLM/llama-output/sentence_transform_complex_dec27_\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6a0d3fca3045eb986f3a6079608617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "model_dir = \"/raid/models/llama2/llama-2-13b-chat/hf\"\n",
    "output_dir = \"/raid/slee3473/LLM/llama-output/sentence_transform_complex_dec27_\"\n",
    "# output_dir = \"/raid/slee3473/LLM/llama-output/sentence_transform_complex_dec25\"\n",
    "device = \"cuda:0\"\n",
    "finetuned = False\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "if os.path.exists(output_dir) and len(os.listdir(output_dir)) > 0: # load pretrained\n",
    "    print(f\"Load a fine-tuned model from {output_dir}\")\n",
    "    model = LlamaForCausalLM.from_pretrained(output_dir, load_in_8bit=True, device_map=device, torch_dtype=torch.float16)\n",
    "    finetuned = True\n",
    "else:\n",
    "    model = LlamaForCausalLM.from_pretrained(model_dir, load_in_8bit=True, device_map=device, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba90980-9248-49c3-983f-ab1229a88068",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2de804f-9ae1-427f-a005-98f2a2a7f56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import os\n",
    "data_dir = \"./../data/\"\n",
    "# train_dataset = datasets.load_from_disk(os.path.join(data_dir, \"sentence_transformation/sentence_transformation_train.hf\"))\n",
    "# test_dataset = datasets.load_from_disk(os.path.join(data_dir, \"sentence_transformation/sentence_transformation_test.hf\"))\n",
    "train_dataset = datasets.load_from_disk(os.path.join(data_dir, \"sentence_transformation_complex/train.hf\"))\n",
    "test_dataset = datasets.load_from_disk(os.path.join(data_dir, \"sentence_transformation_complex/test.hf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3365f1e2-fe6a-4cd3-98ab-8b7c8244bfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(lambda train_dataset: tokenizer(train_dataset[\"text\"], padding='max_length', truncation=True, max_length=64))\n",
    "test_dataset = test_dataset.map(lambda test_dataset: tokenizer(test_dataset[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d7bcf45-44d9-4cd1-acba-b1316543449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.add_column(\"labels\", train_dataset[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f8128a1-2529-46e2-922f-7857bd158461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Capitalize Every Other Letter\n",
      "    For example:\n",
      "    Laughter fills silent rooms. ->  lAuGhTeR FiLlS SiLeNt rOoMs.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(train_dataset[95][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68749707-b53e-4166-857c-9e455cf37a41",
   "metadata": {},
   "source": [
    "## Check base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46e9ddc4-5fa3-41e9-949e-45f76b2440c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/raid/slee3473/Anaconda3/envs/llm/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert Number 1 Between Every Word\n",
      "    Then, Add 'ly' To End of Each Word\n",
      "    For example:\n",
      "    Whispering winds call night. ->  Whisperingly 1ly windsly 1ly callly 1ly night.ly\n"
     ]
    }
   ],
   "source": [
    "# eval_prompt = \"\"\"\n",
    "# When did Russia invade Ukraine?\n",
    "# ---\n",
    "# Answer:\n",
    "# \"\"\"\n",
    "eval_prompt = test_dataset[910][\"prompt\"]\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "981796e5-7778-4704-8e5f-07775259311b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Whisperingly 1ly windsly 1ly callly 1ly night.ly'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[910][\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd015c8-df66-4052-8d90-9ebfd91de1ec",
   "metadata": {},
   "source": [
    "### Prepare model for PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bca662f-c5cb-4978-a22c-d23555a055ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoraConfig(peft_type='LORA', auto_mapping=None, base_model_name_or_path='/raid/models/llama2/llama-2-13b-chat/hf', revision=None, task_type='CAUSAL_LM', inference_mode=False, r=8, target_modules=['q_proj', 'v_proj'], lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)\n",
      "trainable params: 6,553,600 || all params: 13,022,417,920 || trainable%: 0.05032552357220002\n",
      "Insert Number 1 Between Every Word\n",
      "    Then, Add 'ly' To End of Each Word\n",
      "    For example:\n",
      "    Whispering winds call night. ->  Whisperingly 1ly windsly 1ly callly 1ly night.ly\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftConfig, PeftModel, get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=False)\n",
    "\n",
    "if not finetuned:\n",
    "    peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.05, target_modules=[\"q_proj\", \"v_proj\"])\n",
    "    print(peft_config)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "else:\n",
    "    peft_config = PeftConfig.from_pretrained(output_dir)\n",
    "    peft_config.inference_mode = False\n",
    "    print(peft_config)\n",
    "    model = PeftModel.from_pretrained(model, output_dir, is_trainable=True)\n",
    "    \n",
    "model.print_trainable_parameters()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9907629-ab15-4260-b18d-6461c1687926",
   "metadata": {},
   "source": [
    "### Define an optional profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eccb4393-68e1-47fd-b2f2-c3f502866872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback \n",
    "from contextlib import nullcontext \n",
    "enable_profiler = False \n",
    "\n",
    "\n",
    "# if not finetuned:\n",
    "#     config = {\n",
    "#         'lora_config': lora_config,\n",
    "#         'learning_rate': 1e-4,\n",
    "#         'num_train_epochs': 5,\n",
    "#         'gradient_accumulation_steps': 2,\n",
    "#         'per_device_train_batch_size': 2,\n",
    "#         'gradient_checkpointing': False,\n",
    "#     }\n",
    "\n",
    "profiler = nullcontext()\n",
    "\n",
    "# if enable_profiler and not finetuned:\n",
    "#     wait, warmup, active, repeat = 1, 1, 2, 1\n",
    "#     total_steps = (wait + warmup + active) * (1 + repeat)\n",
    "#     schedule = torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=repeat)\n",
    "#     profiler = torch.profiler.profile(\n",
    "#         schedule=schedule, on_trace_ready=torch.profiler.tensorboard_trace_handler(f\"{output_dir}/logs/tensorboard\", record_shapes=True, profile_memory=True, with_stack=True)\n",
    "#     )\n",
    "\n",
    "#     class ProfilerCallback(TrainerCallback):\n",
    "#         def __init__(self, profiler):\n",
    "#             self.profiler = profiler\n",
    "\n",
    "#         def on_step_end(self, *args, **kwargs):\n",
    "#             self.profiler.step()\n",
    "\n",
    "#     profiler_callback = ProfilerCallback(profiler)\n",
    "# else:\n",
    "#     profiler = nullcontext()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc04874-14fa-46fc-821b-a1cedb2817d5",
   "metadata": {},
   "source": [
    "### Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21cf7bbb-673c-44cd-a7bf-ffc84a1c5747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mseongmin_lee\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/slee3473/23-LLMAttribution/experiments/wandb/run-20231227_132453-rmx0oln5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/seongmin_lee/huggingface/runs/rmx0oln5' target=\"_blank\">bumbling-thunder-7</a></strong> to <a href='https://wandb.ai/seongmin_lee/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/seongmin_lee/huggingface' target=\"_blank\">https://wandb.ai/seongmin_lee/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/seongmin_lee/huggingface/runs/rmx0oln5' target=\"_blank\">https://wandb.ai/seongmin_lee/huggingface/runs/rmx0oln5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/slee3473/Anaconda3/envs/llm/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/raid/slee3473/Anaconda3/envs/llm/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='155' max='155' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [155/155 10:23, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>11.747100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>10.589500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>9.337200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>6.977400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.275200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.213500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.772000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.618200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.529900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.480800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.417500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.405800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.381200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.353500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.359900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import default_data_collator, Trainer, TrainingArguments\n",
    "\n",
    "if not finetuned:\n",
    "    config = {\n",
    "        'lora_config': peft_config,\n",
    "        'learning_rate': 1e-4,\n",
    "        'num_train_epochs': 5,\n",
    "        'gradient_accumulation_steps': 2,\n",
    "        'per_device_train_batch_size': 2,\n",
    "        'gradient_checkpointing': False,\n",
    "    }\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir, overwrite_output_dir=True, bf16=True,\n",
    "        logging_dir=f\"{output_dir}/logs\", logging_strategy=\"steps\", logging_steps=10, \n",
    "        save_strategy=\"no\", optim=\"adamw_torch_fused\", max_steps=total_steps if enable_profiler else -1,\n",
    "        **{k: v for k,v in config.items() if k!=\"lora_config\"}\n",
    "    )\n",
    "    \n",
    "    with profiler:\n",
    "        trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, data_collator=default_data_collator, callbacks=[profiler_callback] if enable_profiler else [],)\n",
    "        trainer.train()\n",
    "    \n",
    "    model.save_pretrained(f\"{output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99671b31-1199-4c8f-85fe-e2a3c27467cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Whisperingly 1ly windsly 1ly callly 1ly night.ly</s>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr_prompt = test_dataset[910][\"prompt\"]\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(device)\n",
    "prompt_len = model_input['input_ids'].shape[1]\n",
    "attr_tokens = model.generate(**model_input, max_new_tokens=100)[0]\n",
    "generated_len = attr_tokens.shape[0]\n",
    "attr_str = tokenizer.decode(generated_len, skip_special_tokens=True)\n",
    "attr_token_pos = np.arange(prompt_len, generated_len)  # 39 ... 60\n",
    "\n",
    "tokenizer.decode(attr_tokens[attr_token_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3a41f7e-3281-48d8-8730-543087241b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Whisperingly 1ly windsly 1ly callly 1ly night.ly'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[910][\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30c5556-0930-4510-bc21-b1650b6cc6b7",
   "metadata": {},
   "source": [
    "### Prepare for the attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de5df498-4655-49b8-9600-14b96c662f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_dir = f\"{output_dir}/training_grads_post\"\n",
    "if not os.path.exists(grad_dir):\n",
    "    os.makedirs(grad_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c4cb88f-eff1-4c67-a8d2-9a26650f42ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 1000/1000 [10:34<00:00,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# iterate over training data point\n",
    "model.train()\n",
    "for i, data in enumerate(tqdm(train_dataset)):\n",
    "    # get the Delta_theta when we update the model with \"data\"\n",
    "    input_ids = torch.LongTensor(data[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "    attention_mask = torch.LongTensor(data[\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "    labels = torch.LongTensor(data[\"labels\"]).unsqueeze(0).to(device)\n",
    "    out = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    loss = out.loss\n",
    "    grad_loss = torch.autograd.grad(loss, [param for param in model.parameters() if param.requires_grad])\n",
    "    torch.save(grad_loss, f\"{grad_dir}/{i}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77e021b-df4d-4fed-b8d8-606d20c49916",
   "metadata": {},
   "source": [
    "### Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6a7c0674-535d-4148-aa0c-ff90345942bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' night. call winds Whispering</s>'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attr_prompt = test_dataset[910][\"prompt\"]\n",
    "attr_prompt = test_dataset[0][\"prompt\"]\n",
    "model_input = tokenizer(attr_prompt, return_tensors=\"pt\").to(device)\n",
    "prompt_len = model_input['input_ids'].shape[1]\n",
    "attr_tokens = model.generate(**model_input, max_new_tokens=100)[0]\n",
    "generated_len = attr_tokens.shape[0]\n",
    "attr_str = tokenizer.decode(generated_len, skip_special_tokens=True)\n",
    "attr_token_pos = np.arange(prompt_len, generated_len)  # 39 ... 60\n",
    "\n",
    "tokenizer.decode(attr_tokens[attr_token_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc1a2bed-7e42-47f9-bd25-9c6f10368700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attr_prob_and_grad(model=model, attr_tokens=attr_tokens, attr_token_pos=None, return_named=False):\n",
    "    model.eval()\n",
    "    model.zero_grad()\n",
    "    \n",
    "    attr_tokens = attr_tokens.reshape(1,-1)\n",
    "    attention_mask = torch.ones_like(attr_tokens)\n",
    "    \n",
    "    out = model.base_model(attr_tokens, attention_mask)\n",
    "    attr_logits = out.logits\n",
    "    if attr_token_pos is not None: attr_logits = attr_logits[0, attr_token_pos-1]\n",
    "    attr_probs = F.softmax(attr_logits, dim=1)  # 22 x 32000\n",
    "    attr_probs = attr_probs[torch.arange(len(attr_token_pos)), attr_tokens[0, attr_token_pos].cpu()]\n",
    "    attr_prob = attr_probs.prod()\n",
    "\n",
    "    grad_prob = torch.autograd.grad(attr_prob, [param for param in model.parameters() if param.requires_grad])\n",
    "    model.zero_grad()\n",
    "    \n",
    "    return grad_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5e29c582-6067-4a67-849c-f8148f4ddb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_inner_prod(p1, p2, layerwise=False):\n",
    "    if type(p1) in [list, tuple]:\n",
    "        inner = 0\n",
    "        inners = []\n",
    "        for u,v in zip(p1,p2):\n",
    "            val = torch.sum(u*v).item()\n",
    "            inner += val \n",
    "            inners.append(val)\n",
    "        if layerwise: return inner, inners \n",
    "        else: return inner\n",
    "    elif type(p1) == dict:\n",
    "        inner = dict()\n",
    "        for name in p1:\n",
    "            inner[name] = torch.sum(p1[name]*p2[name]).item()\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e9075cb0-21bb-4d2c-93ba-7ef71b82bccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_prob = get_attr_prob_and_grad(model, attr_tokens, attr_token_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f232ddbc-7a26-44b0-af30-f230c0d52bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [00:27<00:00, 36.08it/s]\n"
     ]
    }
   ],
   "source": [
    "attribution_scores = []\n",
    "for i, data in enumerate(tqdm(train_dataset)):\n",
    "    grad = torch.load(f\"{output_dir}/training_grads_post/{i}.pt\")\n",
    "    inner = get_params_inner_prod(grad, grad_prob)\n",
    "    attribution_scores.append(inner)\n",
    "attribution_scores = np.array(attribution_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8e91670c-32e7-4e55-b1fd-811ffcdc94d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributed = np.argsort(-attribution_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bdc6ce51-c961-471b-bc24-7b547a59c1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([975, 338, 978, 964, 905, 633, 973, 957, 913, 907])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributed[:10] # 951 335 959 362 284 806 930 586 411 386"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "08133212-2c9f-4387-956a-59f184e524ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reverse Order of Words\n",
      "    Then, Add 'ly' To End of Each Word\n",
      "    For example:\n",
      "    Laughter fills silent rooms. -> \n",
      "Replace Vowels with *\n",
      "    For example:\n",
      "    Autumns whisper of change. -> \n",
      "Add 'ly' To End of Each Word\n",
      "    Then, Reverse Order of Words\n",
      "    For example:\n",
      "    Ancient echoes tell tales. -> \n",
      "Double Every Consonant\n",
      "    Then, Capitalize Every Other Letter\n",
      "    For example:\n",
      "    Shadows play tricks nightly. -> \n",
      "Capitalize Every Word\n",
      "    Then, Reverse Order of Words\n",
      "    For example:\n",
      "    Laughter fills silent rooms. -> \n",
      "Add 'ly' To End of Each Word\n",
      "    For example:\n",
      "    Golden horizons promise tomorrow. -> \n",
      "Add 'ly' To End of Each Word\n",
      "    Then, Reverse Order of Words\n",
      "    For example:\n",
      "    Golden horizons promise tomorrow. -> \n",
      "Reverse Order of Words\n",
      "    Then, Remove All Vowels\n",
      "    For example:\n",
      "    Stars twinkle softly above. -> \n",
      "Add 'ly' To End of Each Word\n",
      "    Then, Insert Number 1 Between Every Word\n",
      "    For example:\n",
      "    Golden horizons promise tomorrow. -> \n",
      "Reverse Order of Words\n",
      "    Then, Capitalize Every Word\n",
      "    For example:\n",
      "    Stars twinkle softly above. -> \n"
     ]
    }
   ],
   "source": [
    "for i in attributed[:10]:\n",
    "    print(train_dataset[int(i)]['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3efe5e-e8c3-44d7-b2b3-832105a839ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd0cabd-aaf5-4f79-9723-c7a06b02508f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f6465db-7508-4b37-b8da-cc99c8d39a50",
   "metadata": {},
   "source": [
    "### Try fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df1205c2-b8f2-4395-8e67-c1d48813eba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert Number 1 Between Every Word\n",
      "    Then, Add 'ly' To End of Each Word\n",
      "    For example:\n",
      "    Whispering winds call night. ->  Whisperingly 1ly windsly 1ly callly 1ly night.ly\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d5ba855-0ab2-49fc-99e9-fe2eb9b0ea13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Insert Number 1 Between Every Word\\n    Then, Add 'ly' To End of Each Word\\n    For example:\\n    Whispering winds call night. -> \""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6b8174-1762-4f57-b5ee-31900d2abf03",
   "metadata": {},
   "source": [
    "### Sub-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0dfa9e9-8be3-4483-937a-0906715e43a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_param(model, params):\n",
    "    if type(params) in [list, tuple]:\n",
    "        for model_param, input_param in zip(model.parameters(), params):\n",
    "            model_param.data = torch.Tensor(input_param).to(device)\n",
    "    elif type(params) == dict:\n",
    "        for name, param in model.named_parameters():\n",
    "            param.data = torch.Tensor(params[name]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f418bd97-6adb-4583-be0a-474f296f85cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85a093e-836d-4606-9e6a-4270cb2c3895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc5646a5-1cfc-43ca-a823-ff370431fd57",
   "metadata": {},
   "source": [
    "### Attribute using gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f78994fb-8411-455d-a4d6-a76119587889",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "attr_prompt = eval_prompt\n",
    "prompt_len = model_input['input_ids'].shape[1]\n",
    "attr_tokens = model.generate(**model_input, max_new_tokens=100)[0]\n",
    "generated_len = attr_tokens.shape[0]\n",
    "attr_str = tokenizer.decode(generated_len, skip_special_tokens=True)\n",
    "attr_token_pos = np.arange(prompt_len, generated_len)  # 39 ... 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e61731fa-d8ca-4e48-a20f-f5a2874f2f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Whisperingly 1ly windsly 1ly callly 1ly night.ly</s>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(attr_tokens[attr_token_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51cf8241-ccaa-409f-837e-e3fb3c8d050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attr_prob_and_grad(model=model, attr_tokens=attr_tokens, attr_token_pos=None, return_named=False):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    \n",
    "    attr_tokens = attr_tokens.reshape(1,-1)\n",
    "    attention_mask = torch.ones_like(attr_tokens)\n",
    "    out = model(attr_tokens, attention_mask)\n",
    "    attr_logits = out.logits\n",
    "    if attr_token_pos is not None: attr_logits = attr_logits[0, attr_token_pos-1]\n",
    "    attr_probs = F.softmax(attr_logits, dim=1)  # 22 x 32000\n",
    "    attr_probs = attr_probs[torch.arange(len(attr_token_pos)), attr_tokens[0, attr_token_pos].cpu()]\n",
    "    attr_prob = attr_probs.prod()\n",
    "\n",
    "    grad_prob = torch.autograd.grad(attr_prob, [param for param in model.parameters() if param.requires_grad])\n",
    "    model.zero_grad()\n",
    "    model.eval()\n",
    "    \n",
    "    return grad_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82ecf99a-539c-4428-b662-bf469f65acac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 1000/1000 [10:04<00:00,  1.65it/s]\n"
     ]
    }
   ],
   "source": [
    "grad_prob = get_attr_prob_and_grad(attr_token_pos=attr_token_pos) # get the gradient for the attr_tokens\n",
    "inners = []\n",
    "layerwise_inners = []\n",
    "\n",
    "# iterate over training data point\n",
    "for data in tqdm(train_dataset):\n",
    "    # get the Delta_theta when we update the model with \"data\"\n",
    "    input_ids = torch.LongTensor(data[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "    attention_mask = torch.LongTensor(data[\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "    labels = torch.LongTensor(data[\"labels\"]).unsqueeze(0).to(device)\n",
    "    out = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    loss = out.loss\n",
    "    grad_loss = torch.autograd.grad(loss, [param for param in model.parameters() if param.requires_grad])\n",
    "    inner, inner_layerwise = get_params_inner_prod(grad_prob, grad_loss, layerwise=True)\n",
    "    inners.append(inner)\n",
    "    layerwise_inners.append(inner_layerwise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9045c848-eb4a-4e3e-8056-eab69a288e31",
   "metadata": {},
   "source": [
    "### Layerwise inner product and Overall inner product --- Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "821d39eb-3566-4cf6-8e4d-8775480fcfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 160)\n"
     ]
    }
   ],
   "source": [
    "layerwise_inners = np.array(layerwise_inners)\n",
    "print(layerwise_inners.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dd7c91e1-d98f-4a11-a882-cb6030ff0956",
   "metadata": {},
   "outputs": [],
   "source": [
    "layerwise_inners = layerwise_inners.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb356118-315e-41de-b0d0-1b19e7c8a637",
   "metadata": {},
   "outputs": [],
   "source": [
    "inners = np.array(inners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a866911-6fb3-4a1d-961a-8d32dfde02a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 160) (1000,)\n"
     ]
    }
   ],
   "source": [
    "print(layerwise_inners.shape, inners.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bfec2fa5-c12a-40b2-b0f0-1ce5c61e377d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 10)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_main = np.arange(20).reshape(2,10)\n",
    "temp = np.zeros(10)\n",
    "np.vstack([temp_main, temp]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4dd1cbc2-14be-4fb2-bd68-9315d4d4fff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(161, 1000)\n"
     ]
    }
   ],
   "source": [
    "inners_all = np.vstack([layerwise_inners, inners])\n",
    "print(inners_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "63b74060-10fc-4bc1-8247-38a1bc9d5774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.        , -0.68653015, -0.6655457 , -0.65754248, -0.64918273,\n",
       "       -0.6469869 , -0.64345741, -0.6330343 , -0.626424  , -0.6114907 ])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.corrcoef(inners, layerwise_inners[:,-1])\n",
    "corr_all = np.corrcoef(inners_all) # abs?\n",
    "corr = np.abs(corr_all[:,-1])\n",
    "np.sort(-corr)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d94c0ae-d4e4-4e2b-af68-87a9cbb15f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.95520312e-01, -6.28454125e-03,  6.46986896e-01,  6.86530149e-01,\n",
       "        2.85992342e-01,  2.98185156e-01,  2.26502953e-01,  5.84417952e-01,\n",
       "        2.29343894e-01,  4.48984757e-01, -1.08599970e-01,  4.81853092e-01,\n",
       "        1.35998382e-01,  1.22757800e-01, -1.72662109e-01,  5.86475204e-01,\n",
       "        7.61374691e-02,  3.36693579e-01,  1.76516055e-02,  6.57542484e-01,\n",
       "        2.00970921e-01,  1.56813666e-01,  7.11017322e-02,  5.46087527e-01,\n",
       "        1.82512550e-01,  3.01574109e-01,  1.36031047e-01,  5.28923374e-01,\n",
       "        3.66728993e-01,  4.49987237e-01, -1.44642960e-01,  6.26424003e-01,\n",
       "        9.07436437e-02,  1.91303429e-01,  5.87858073e-02,  6.11490700e-01,\n",
       "        2.25806195e-01,  3.40868535e-01,  1.70679809e-01,  6.33034302e-01,\n",
       "        2.41859695e-01,  4.06385714e-01,  2.83346041e-01,  6.43457407e-01,\n",
       "       -8.60242704e-03,  2.45595627e-01,  1.90254631e-01,  6.49182728e-01,\n",
       "        1.24567382e-01,  3.95073393e-01,  2.61398059e-01,  6.65545695e-01,\n",
       "        3.66704760e-01,  3.65403928e-01,  2.05293974e-01,  6.03289237e-01,\n",
       "        8.24877653e-02,  2.95605494e-01,  2.64581328e-02,  5.38768296e-01,\n",
       "        2.56905493e-01,  1.60614257e-01,  2.25763545e-01,  6.06192513e-01,\n",
       "        1.53415644e-01,  1.32105313e-01,  8.06895353e-02,  4.99793850e-01,\n",
       "        2.19859324e-01,  3.30856759e-01,  1.57074035e-01,  4.51191615e-01,\n",
       "        7.93019748e-02,  2.42972322e-01,  2.09203130e-02,  4.45917356e-01,\n",
       "        6.11337410e-02,  1.55789075e-01,  1.41172835e-01,  5.53241752e-01,\n",
       "        1.29695673e-01,  2.00935109e-01,  8.20361943e-02,  3.96066861e-01,\n",
       "        1.31188566e-02,  7.59284148e-02,  8.41654035e-02,  4.26443526e-01,\n",
       "        9.32888710e-02,  2.81531001e-02, -1.83503719e-01,  3.06236412e-01,\n",
       "        1.70960340e-01,  2.00509404e-01, -8.76180982e-02,  4.15494871e-01,\n",
       "        7.29829091e-02,  1.83523238e-01,  1.19751965e-01,  4.84863591e-01,\n",
       "        2.40028036e-01,  3.56209014e-01, -3.68277898e-02,  4.89099295e-01,\n",
       "        1.16044131e-02,  5.47778846e-02, -1.24293623e-01,  4.27198219e-01,\n",
       "        1.01890383e-01,  9.41515959e-02, -1.24664229e-01,  3.51567046e-01,\n",
       "        6.57169634e-02,  9.56645350e-02, -3.25684430e-02,  4.00173642e-01,\n",
       "        5.34401407e-02, -8.03333204e-02, -6.30114723e-02,  2.79256669e-01,\n",
       "        3.84297761e-02,  9.11342541e-02, -6.25075267e-02,  3.39893031e-01,\n",
       "        7.50886977e-02,  1.08755523e-02, -1.78138429e-01,  3.99277603e-01,\n",
       "       -2.30199321e-02,  1.02633096e-01, -2.38984462e-01,  2.42190563e-01,\n",
       "        1.83169360e-01,  1.04406281e-01,  9.72371723e-02,  3.78729098e-01,\n",
       "        8.99707237e-02,  1.73438676e-01,  2.42172123e-01,  3.01533799e-01,\n",
       "        4.69893665e-02,  4.36230723e-02, -2.67519615e-02,  2.89517254e-01,\n",
       "        1.33914024e-01,  1.82884069e-01,  1.13009685e-01,  2.04201310e-01,\n",
       "        1.55544397e-02,  8.87492647e-02,  2.02420362e-01,  1.96712852e-01,\n",
       "       -1.57073439e-04,  8.76879035e-02,  1.40848562e-01,  2.08678974e-01,\n",
       "        2.61322987e-02,  1.17942734e-02, -1.53214052e-01,  3.04858657e-01,\n",
       "        1.00000000e+00])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af645e8e-057b-4bfc-bde9-0dc66e9d9955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003839194541797042"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_layers = layerwise_inners.shape[1]\n",
    "for layer_i in range(num_layers):\n",
    "    # Check correlations between layerwise_inners[:][layer_i] and inners[:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e16683d-1bbb-4744-89b5-b5c948df92b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a517de2e-2de8-467d-b294-6ee05a826763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2984e0-67ab-4582-9a86-28a87c69387c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709b1227-5504-49cd-935d-db01b055613f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28e957b-3ebb-45f4-a15a-485ca5ea98fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "37ff2b23-2f48-4c55-b3ce-bd04386d39eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[951 335 959 362 284 806 930 586 411 386]\n"
     ]
    }
   ],
   "source": [
    "inners = np.array(inners)\n",
    "print(np.argsort(-inners)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bd536f2e-6463-4abb-a0f7-13dc440b0e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove All Vowels\n",
      "    Then, Reverse Order of Words\n",
      "    For example:\n",
      "    Gentle waves kiss shores. -> \n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[959][\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab9a21d-f0ef-49aa-8ad8-4fe37c4d06d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad795972-c790-4cc9-9a50-703ba5ad6253",
   "metadata": {},
   "source": [
    "### Timing Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e4ddd59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d8993d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over training data point\n",
    "i = 0\n",
    "data = train_dataset[i]\n",
    "# get the Delta_theta when we update the model with \"data\"\n",
    "input_ids = torch.LongTensor(data[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "attention_mask = torch.LongTensor(data[\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "labels = torch.LongTensor(data[\"labels\"]).unsqueeze(0).to(device)\n",
    "start = time.time()\n",
    "out = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "inference_time = time.time() - start\n",
    "loss = out.loss\n",
    "start = time.time()\n",
    "grad_loss = torch.autograd.grad(loss, [param for param in model.parameters() if param.requires_grad])\n",
    "grad_time = time.time() - start\n",
    "start = time.time()\n",
    "_ = get_params_inner_prod(grad_prob, grad_loss, layerwise=True)\n",
    "inner_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7d60de1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3847188949584961 0.3336656093597412 0.06148576736450195\n"
     ]
    }
   ],
   "source": [
    "print(inference_time, grad_time, inner_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0876166a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425c136c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05569835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5815a6b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611216c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5fb62f7-2d73-49b2-a14d-641fe19c2029",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136b9690-c8d1-405c-b5f1-45240aa6e7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
